{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29999613-06e4-41c4-887e-b1b5fb71799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa6d1d6-dad2-47b0-8199-590d5fd51e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873446e2-566c-49ff-a03b-40b216603cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n",
      "Initializing. Generating 10000 training/testing examples from: data/usher_barcodes.csv...\n",
      "Generated 1000 training examples...\n",
      "Generated 2000 training examples...\n",
      "Generated 3000 training examples...\n",
      "Generated 4000 training examples...\n",
      "Generated 5000 training examples...\n",
      "Generated 6000 training examples...\n",
      "Generated 7000 training examples...\n",
      "Generated 8000 training examples...\n",
      "Generated 9000 training examples...\n",
      "Elapsed: 33.87837815284729 seconds.\n",
      "Shape of known_freqs tensor: torch.Size([10000, 3937])\n",
      "Shape of snv_freqs tensor: torch.Size([10000, 6520])\n",
      "Training set size: 8000\n",
      "Validation set size: 1000\n",
      "Test set size: 1000\n",
      "Known frequencies batch shape: torch.Size([1000, 3937])\n",
      "SNV frequencies batch shape: torch.Size([1000, 6520])\n",
      "First batch - SNV frequencies: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1030, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2422, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3201, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1385, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3961, 0.0000]],\n",
      "       device='mps:0')\n",
      "First batch - Known frequencies: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0652, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='mps:0')\n",
      "First batch - Known frequencies (non-zero) 0: tensor([0.1215, 0.2529, 0.0368, 0.1341, 0.1859, 0.1888, 0.0743, 0.0057],\n",
      "       device='mps:0')\n",
      "\n",
      "\n",
      "Validation Known frequencies batch shape: torch.Size([1000, 3937])\n",
      "Validation SNV frequencies batch shape: torch.Size([1000, 6520])\n",
      "Validation batch - SNV frequencies: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.2044, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0010, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1617, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1285, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.4208, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1705, 0.0000]],\n",
      "       device='mps:0')\n",
      "Validation batch - Known frequencies: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0532,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "       device='mps:0')\n",
      "Validation batch - Known frequencies (non-zero) 0: tensor([0.0558, 0.1897, 0.0222, 0.0934, 0.1529, 0.1322, 0.0861, 0.1955, 0.0008,\n",
      "        0.0713], device='mps:0')\n",
      "\n",
      "\n",
      "Test Known frequencies batch shape: torch.Size([1000, 3937])\n",
      "Test SNV frequencies batch shape: torch.Size([1000, 6520])\n",
      "Test batch - SNV frequencies: tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3197, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1637, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.1852, 0.0000],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3602, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.3164, 0.0000],\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0926, 0.0000]],\n",
      "       device='mps:0')\n",
      "Test batch - Known frequencies: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], device='mps:0')\n",
      "Test batch - Known frequencies (non-zero) 0: tensor([0.0528, 0.0328, 0.0304, 0.0118, 0.0378, 0.0446, 0.0504, 0.0063, 0.0869,\n",
      "        0.0264, 0.0429, 0.0787, 0.0316, 0.0752, 0.0154, 0.0137, 0.0278, 0.0248,\n",
      "        0.0641, 0.0434, 0.0642, 0.0584, 0.0568, 0.0018, 0.0213],\n",
      "       device='mps:0')\n",
      "\n",
      "Using mps device\n",
      "Epoch 1, Train loss: 0.075027, Validation loss: 0.000163\n",
      "Epoch 2, Train loss: 0.030945, Validation loss: 0.000173\n",
      "Epoch 3, Train loss: 0.016994, Validation loss: 0.000227\n",
      "Epoch 4, Train loss: 0.010464, Validation loss: 0.000365\n",
      "Epoch 5, Train loss: 0.007104, Validation loss: 0.000644\n",
      "Epoch 6, Train loss: 0.005245, Validation loss: 0.001162\n",
      "Epoch 7, Train loss: 0.004135, Validation loss: 0.001875\n",
      "Epoch 8, Train loss: 0.003420, Validation loss: 0.002555\n",
      "Epoch 9, Train loss: 0.002927, Validation loss: 0.003002\n",
      "Epoch 10, Train loss: 0.002565, Validation loss: 0.003199\n",
      "Epoch 11, Train loss: 0.002285, Validation loss: 0.003180\n",
      "Epoch 12, Train loss: 0.002060, Validation loss: 0.003023\n",
      "Epoch 13, Train loss: 0.001874, Validation loss: 0.002817\n",
      "Epoch 14, Train loss: 0.001718, Validation loss: 0.002613\n",
      "Epoch 15, Train loss: 0.001584, Validation loss: 0.002429\n",
      "Epoch 16, Train loss: 0.001468, Validation loss: 0.002268\n",
      "Epoch 17, Train loss: 0.001366, Validation loss: 0.002127\n",
      "Epoch 18, Train loss: 0.001276, Validation loss: 0.002004\n",
      "Epoch 19, Train loss: 0.001196, Validation loss: 0.001895\n",
      "Epoch 20, Train loss: 0.001124, Validation loss: 0.001798\n",
      "Epoch 21, Train loss: 0.001060, Validation loss: 0.001711\n",
      "Epoch 22, Train loss: 0.001002, Validation loss: 0.001632\n",
      "Epoch 23, Train loss: 0.000949, Validation loss: 0.001560\n",
      "Epoch 24, Train loss: 0.000901, Validation loss: 0.001494\n",
      "Epoch 25, Train loss: 0.000857, Validation loss: 0.001434\n",
      "Epoch 26, Train loss: 0.000817, Validation loss: 0.001379\n",
      "Epoch 27, Train loss: 0.000780, Validation loss: 0.001328\n",
      "Epoch 28, Train loss: 0.000745, Validation loss: 0.001281\n",
      "Epoch 29, Train loss: 0.000714, Validation loss: 0.001237\n",
      "Epoch 30, Train loss: 0.000684, Validation loss: 0.001196\n",
      "Epoch 31, Train loss: 0.000657, Validation loss: 0.001158\n",
      "Epoch 32, Train loss: 0.000631, Validation loss: 0.001123\n",
      "Epoch 33, Train loss: 0.000607, Validation loss: 0.001090\n",
      "Epoch 34, Train loss: 0.000585, Validation loss: 0.001059\n",
      "Epoch 35, Train loss: 0.000564, Validation loss: 0.001029\n",
      "Epoch 36, Train loss: 0.000545, Validation loss: 0.001002\n",
      "Epoch 37, Train loss: 0.000526, Validation loss: 0.000976\n",
      "Epoch 38, Train loss: 0.000509, Validation loss: 0.000951\n",
      "Epoch 39, Train loss: 0.000492, Validation loss: 0.000928\n",
      "Epoch 40, Train loss: 0.000477, Validation loss: 0.000906\n",
      "Epoch 41, Train loss: 0.000462, Validation loss: 0.000885\n",
      "Epoch 42, Train loss: 0.000449, Validation loss: 0.000865\n",
      "Epoch 43, Train loss: 0.000436, Validation loss: 0.000846\n",
      "Epoch 44, Train loss: 0.000423, Validation loss: 0.000827\n",
      "Epoch 45, Train loss: 0.000411, Validation loss: 0.000810\n",
      "Epoch 46, Train loss: 0.000400, Validation loss: 0.000794\n",
      "Epoch 47, Train loss: 0.000389, Validation loss: 0.000778\n",
      "Epoch 48, Train loss: 0.000379, Validation loss: 0.000762\n",
      "Epoch 49, Train loss: 0.000369, Validation loss: 0.000748\n",
      "Epoch 50, Train loss: 0.000360, Validation loss: 0.000734\n",
      "Epoch 51, Train loss: 0.000351, Validation loss: 0.000720\n",
      "Epoch 52, Train loss: 0.000343, Validation loss: 0.000707\n",
      "Epoch 53, Train loss: 0.000334, Validation loss: 0.000695\n",
      "Epoch 54, Train loss: 0.000327, Validation loss: 0.000683\n",
      "Epoch 55, Train loss: 0.000319, Validation loss: 0.000671\n",
      "Epoch 56, Train loss: 0.000312, Validation loss: 0.000660\n",
      "Epoch 57, Train loss: 0.000305, Validation loss: 0.000649\n",
      "Epoch 58, Train loss: 0.000298, Validation loss: 0.000639\n",
      "Epoch 59, Train loss: 0.000292, Validation loss: 0.000629\n",
      "Epoch 60, Train loss: 0.000285, Validation loss: 0.000619\n",
      "Epoch 61, Train loss: 0.000279, Validation loss: 0.000610\n",
      "Epoch 62, Train loss: 0.000274, Validation loss: 0.000600\n",
      "Epoch 63, Train loss: 0.000268, Validation loss: 0.000592\n",
      "Epoch 64, Train loss: 0.000263, Validation loss: 0.000583\n",
      "Epoch 65, Train loss: 0.000258, Validation loss: 0.000575\n",
      "Epoch 66, Train loss: 0.000253, Validation loss: 0.000567\n",
      "Epoch 67, Train loss: 0.000248, Validation loss: 0.000559\n",
      "Epoch 68, Train loss: 0.000243, Validation loss: 0.000551\n",
      "Epoch 69, Train loss: 0.000239, Validation loss: 0.000544\n",
      "Epoch 70, Train loss: 0.000234, Validation loss: 0.000536\n",
      "Epoch 71, Train loss: 0.000230, Validation loss: 0.000530\n",
      "Epoch 72, Train loss: 0.000226, Validation loss: 0.000523\n",
      "Epoch 73, Train loss: 0.000222, Validation loss: 0.000516\n",
      "Epoch 74, Train loss: 0.000218, Validation loss: 0.000510\n",
      "Epoch 75, Train loss: 0.000215, Validation loss: 0.000504\n",
      "Epoch 76, Train loss: 0.000211, Validation loss: 0.000498\n",
      "Epoch 77, Train loss: 0.000208, Validation loss: 0.000492\n",
      "Epoch 78, Train loss: 0.000204, Validation loss: 0.000486\n",
      "Epoch 79, Train loss: 0.000201, Validation loss: 0.000480\n",
      "Epoch 80, Train loss: 0.000198, Validation loss: 0.000475\n",
      "Epoch 81, Train loss: 0.000195, Validation loss: 0.000470\n",
      "Epoch 82, Train loss: 0.000192, Validation loss: 0.000465\n",
      "Epoch 83, Train loss: 0.000189, Validation loss: 0.000460\n",
      "Epoch 84, Train loss: 0.000186, Validation loss: 0.000455\n",
      "Epoch 85, Train loss: 0.000184, Validation loss: 0.000450\n",
      "Epoch 86, Train loss: 0.000181, Validation loss: 0.000445\n",
      "Epoch 87, Train loss: 0.000178, Validation loss: 0.000441\n",
      "Epoch 88, Train loss: 0.000176, Validation loss: 0.000436\n",
      "Epoch 89, Train loss: 0.000173, Validation loss: 0.000432\n",
      "Epoch 90, Train loss: 0.000171, Validation loss: 0.000427\n",
      "Epoch 91, Train loss: 0.000168, Validation loss: 0.000423\n",
      "Epoch 92, Train loss: 0.000166, Validation loss: 0.000419\n",
      "Epoch 93, Train loss: 0.000164, Validation loss: 0.000415\n",
      "Epoch 94, Train loss: 0.000162, Validation loss: 0.000411\n",
      "Epoch 95, Train loss: 0.000160, Validation loss: 0.000407\n",
      "Epoch 96, Train loss: 0.000157, Validation loss: 0.000404\n",
      "Epoch 97, Train loss: 0.000155, Validation loss: 0.000400\n",
      "Epoch 98, Train loss: 0.000154, Validation loss: 0.000397\n",
      "Epoch 99, Train loss: 0.000152, Validation loss: 0.000394\n",
      "Epoch 100, Train loss: 0.000151, Validation loss: 0.000390\n",
      "Epoch 101, Train loss: 0.000149, Validation loss: 0.000387\n",
      "Early stopping on epoch 101\n",
      "Test Error: \n",
      " Avg loss: 0.000387 \n",
      "\n",
      "[[-0.0095735   0.00075858 -0.02343915 ... -0.00252937 -0.00353644\n",
      "   0.00697723]\n",
      " [-0.00378961  0.000434    0.00723459 ...  0.00264818  0.00555949\n",
      "   0.00536127]\n",
      " [ 0.00154668  0.02026244 -0.00034372 ...  0.0143909   0.00123039\n",
      "   0.00238798]\n",
      " ...\n",
      " [ 0.02903348 -0.01210501 -0.03777427 ...  0.00197278  0.01223265\n",
      "  -0.03431847]\n",
      " [-0.01507706  0.00206395 -0.01466853 ...  0.00434693 -0.03336412\n",
      "  -0.00412248]\n",
      " [ 0.0043859  -0.03699717 -0.00073817 ...  0.01969332 -0.00687895\n",
      "  -0.02446378]]\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "model_demo.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c9b32ff-82ff-42ab-8949-057f0c9ba957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "#user hardware accelerator if available\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db710dd5-16f3-4c78-a31a-067da49ccea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import generate_training_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8856bbac-22c1-4d29-98a9-5dcda7ad2824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising. Generating 100 training/testing examples from: data/usher_barcodes.csv...\n",
      "Generated 0 training examples...\n",
      "Generated 50 training examples...\n",
      "Elapsed: 2.0254271189371744 minutes.\n"
     ]
    }
   ],
   "source": [
    "#generate training examples\n",
    "known_freqs, sorted_snv_freqs=generate_training_examples('data/usher_barcodes.csv', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "730640fc-9520-49f9-9ad4-9ca00da54b02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(known_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "205bd9b9-0dd9-4bfe-8ec2-3b1773b74656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising. Generating 1000 training/testing examples from: data/usher_barcodes.csv...\n",
      "Generated 0 training examples...\n",
      "Generated 50 training examples...\n",
      "Generated 100 training examples...\n",
      "Generated 150 training examples...\n",
      "Generated 200 training examples...\n",
      "Generated 250 training examples...\n",
      "Generated 300 training examples...\n",
      "Generated 350 training examples...\n",
      "Generated 400 training examples...\n",
      "Generated 450 training examples...\n",
      "Generated 500 training examples...\n",
      "Generated 550 training examples...\n",
      "Generated 600 training examples...\n",
      "Generated 650 training examples...\n",
      "Generated 700 training examples...\n",
      "Generated 750 training examples...\n",
      "Generated 800 training examples...\n",
      "Generated 850 training examples...\n",
      "Generated 900 training examples...\n",
      "Generated 950 training examples...\n",
      "Elapsed: 20.347839613755543 minutes.\n"
     ]
    }
   ],
   "source": [
    "#generating 1000 training examples\n",
    "known_freqs, sorted_snv_freqs=generate_training_examples('data/usher_barcodes.csv', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "643cc14c-9232-45c0-9528-074f06c1aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_to_tensor(data, pad_value=0):\n",
    "    '''Helper function to pad the input/output data and convert them to PyTorch Tensors'''\n",
    "    max_len = max(len(sublist) for sublist in data)\n",
    "    padded_list = [sublist + [pad_value] * (max_len - len(sublist)) for sublist in data]\n",
    "    torch_tensor = torch.tensor(padded_list)\n",
    "\n",
    "    return torch_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9151ee9a-0aeb-4c40-8f2e-a230aad2cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=padding_to_tensor(sorted_snv_freqs)\n",
    "y=padding_to_tensor(known_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "975e10b8-714c-4def-9ae1-1ac99660e30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Identity(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.linear_relu_stack(X)\n",
    "        return out\n",
    "\n",
    "input_dim = 100  # Length of SNV frequency vector\n",
    "output_dim = 100  # Length of known variant frequency vector\n",
    "hidden_dim = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f5b846c-46a0-4636-aa83-62d5d409f0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = InitNetwork().to(device)\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9eeb2142-c344-4c1d-85cd-ecb218d0c673",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "dataset = torch.utils.data.TensorDataset(X,y)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "868bdb47-b867-4138-963e-2a7f317062b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS device does not support linear for non-float inputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (inputs, targets) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[0;32m----> 3\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[1;32m      6\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ww/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ww/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 11\u001b[0m, in \u001b[0;36mInitNetwork.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m---> 11\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_relu_stack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ww/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ww/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ww/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ww/lib/python3.9/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ww/lib/python3.9/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/ww/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS device does not support linear for non-float inputs"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
